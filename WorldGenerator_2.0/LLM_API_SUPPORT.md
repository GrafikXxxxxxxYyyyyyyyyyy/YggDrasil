# Поддержка моделей через API-провайдеров (без локальных весов)

**Требование:** фреймворк должен обеспечивать **поддержку использования облачных сервисов и API-провайдеров на уровне любой модели**, для которой такая возможность реализована. То есть **любой блок** (LLM, VLM, эмбеддинги, генерация изображений по API и т.д.) **должен иметь возможность работать через API провайдера**, если для данного типа модели такой способ доступен — без скачивания и загрузки весов для инференса.

Канон: [SCALABILITY_AND_EXTENSIBILITY.md](SCALABILITY_AND_EXTENSIBILITY.md), [Abstract_Task_Nodes.md](Abstract_Task_Nodes.md), [MULTI_ENDPOINT_DEPLOYMENT.md](MULTI_ENDPOINT_DEPLOYMENT.md).

---

## 1. Цель и область применения

- **Любой блок — возможность через API:** использование облачных и API-доступных моделей должно поддерживаться **на уровне любого типа блока**, для которого провайдер предоставляет API: LLM, VLM, текстовые эмбеддинги, image-to-image/text-to-image по API (например, DALL·E, Stable Diffusion API), речевые модели и т.д. Вызов инференса через HTTP/API провайдера, без локального хранения весов; выбор «локально vs API» задаётся конфигом блока (`backend: "api"`, `provider`, `model_id`, `endpoint_url`).
- **LLM как основной пример:** документ детализирует требования на примере **LLM** (Qwen, OpenAI, Anthropic и т.д.); те же принципы применяются к любым другим типам моделей с API (VLM по API, эмбеддинги по API, генерация изображений по API).
- **Единый контракт:** узел-задача в графе имеет одни и те же порты и контракт выполнения независимо от того, реализован ли блок локально или через API. Граф/пайплайн/этап/мир не зависят от способа выполнения блока.

---

## 2. Требования к реализации

### 2.1 Блоки LLM с режимом «только API»

- [ ] **Режим инференса без локальных весов:** блок (узел-задача LLM) может быть сконфигурирован для работы **исключительно через API**: указание провайдера (или endpoint_url), идентификатор модели, при необходимости ключ/учётные данные. При вызове `forward`/`run` блок выполняет HTTP-запрос к API и возвращает ответ; **никакой загрузки весов в память, никакой локальной инициализации модели**.
- [ ] **Контракт портов:** как у обычного LLM-блока — входы (промпт, контекст, опционально системный промпт, параметры генерации), выход (текст или поток токенов). Контракт единый для локального и API-режима, чтобы граф/пайплайн не зависели от способа выполнения.
- [ ] **Поддерживаемые провайдеры/форматы:** как минимум возможность подключения к OpenAI-совместимому API и к провайдерам вроде Anthropic; при необходимости адаптеры под разные форматы запросов/ответов. Конкретный перечень моделей (QwenMax, ChatGPT 5.2, Opus 4 и т.д.) определяется доступностью у провайдера и задаётся в конфиге как `model_id` (или аналог).

### 2.2 Конфигурация и безопасность

- [ ] **Конфиг блока:** параметры типа `provider` (или `backend: "api"`), `model_id`, `endpoint_url` (если свой эндпоинт), `api_key` или ссылка на секрет (переменная окружения, секретное хранилище). Без хардкода ключей в репозитории; документировать рекомендуемый способ подачи учётных данных.
- [ ] **Сериализация:** в конфиг графа/пайплайна не попадают секреты; в конфиге хранятся только идентификаторы (provider, model_id, endpoint_url при необходимости). Загрузка ключей — при инициализации блока из окружения или из защищённого хранилища.

### 2.3 Интеграция с уровнями фреймворка

- [ ] **Граф:** LLM-блок в режиме API регистрируется в реестре как тот же тип узла-задачи (например, `llm/openai`, `llm/anthropic`, `llm/api`); сборка графа через add_node с конфигом API. Исполнитель графа вызывает block.forward(inputs) — внутри блок делает запрос к API.
- [ ] **Пайплайн и multi-endpoint:** узел пайплайна может быть графом с LLM-блоком в режиме API; альтернативно сам граф может быть развёрнут на эндпоинте, а LLM внутри него — по API. Связь с [MULTI_ENDPOINT_DEPLOYMENT.md](MULTI_ENDPOINT_DEPLOYMENT.md): консистентность контрактов и сериализации.
- [ ] **Этап и мир:** этапы (Автор, Философ, Архитектор и т.д.) могут использовать LLM полностью через API; state и порядок выполнения не зависят от того, локальный это LLM или API.

---

## 3. Критерии приёмки

- [ ] В граф можно добавить узел LLM, сконфигурированный **только на API** (без путей к чекпоинтам, без загрузки весов). Вызов run(graph, inputs) выполняет инференс через выбранный API и возвращает выход по контракту портов.
- [ ] Поддерживаются как минимум один OpenAI-совместимый провайдер и при необходимости Anthropic (или иной документированный API). Конкретные модели (QwenMax, GPT-5.2, Opus 4 и т.д.) задаются через model_id в конфиге в соответствии с именованием провайдера.
- [ ] Документация и примеры: как задать блок LLM в режиме API в конфиге, как передать учётные данные безопасно, как использовать такой граф в пайплайне и в цикле мира.

---

## 4. Связь с другими документами

| Документ | Связь |
|----------|--------|
| [SCALABILITY_AND_EXTENSIBILITY.md](SCALABILITY_AND_EXTENSIBILITY.md) | LLM перечислены как поддерживаемый класс моделей; данный документ уточняет, что поддержка включает режим «только API» без локальных весов. |
| [MULTI_ENDPOINT_DEPLOYMENT.md](MULTI_ENDPOINT_DEPLOYMENT.md) | Граф на эндпоинте и граф с LLM по API — согласованные контракты; оркестратор не различает источник выполнения по контракту. |
| [TODO_07_FUTURE_AND_IMPROVEMENTS.md](TODO_07_FUTURE_AND_IMPROVEMENTS.md) | Бэкенды и LLM; данный документ фиксирует требование «LLM через API» как полноценную возможность фреймворка. |

Итого: **фреймворк должен реализовать полную поддержку использования моделей через API без скачивания и локальной инициализации весов** — для **любого типа блока**, для которого у провайдера есть API (LLM, VLM, эмбеддинги, генерация изображений и т.д.). В частности, для LLM (QwenMax, ChatGPT, Opus и др.) — как в данном документе; для остальных типов — тот же контракт (backend: "api", provider, model_id) и единый интерфейс блока.

Обучение, LoRA и повторное использование моделей при работе через API и при общих весах описаны в [TRAINING_REUSE_AND_API_SCENARIOS.md](TRAINING_REUSE_AND_API_SCENARIOS.md).
