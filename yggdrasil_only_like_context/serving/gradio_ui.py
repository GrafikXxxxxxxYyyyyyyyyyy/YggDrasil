# yggdrasil/serving/gradio_ui.py
"""Gradio UI –¥–ª—è YggDrasil ‚Äî –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ª—é–±—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (image, video, audio).

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –í—ã–±–æ—Ä –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ (–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ / –í–∏–¥–µ–æ / –ê—É–¥–∏–æ) –∏ —à–∞–±–ª–æ–Ω–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞
- –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤—Ö–æ–¥—ã –ø–æ –≥—Ä–∞—Ñ—É (prompt, control_image, num_frames –∏ —Ç.–¥.)
- –ü—Ä–µ—Å–µ—Ç—ã —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å–µ–º—è, –±–∞—Ç—á, —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –∏ –ø–æ–Ω—è—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö
"""
from __future__ import annotations

import io
import time
import random
import torch
import numpy as np
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from PIL import Image

from .schema import ServerConfig
from .param_utils import merge_extra_params_json, infer_input_visibility


# ==================== HELPERS ====================

def _get_device_info() -> str:
    if torch.cuda.is_available():
        name = torch.cuda.get_device_name(0)
        props = torch.cuda.get_device_properties(0)
        mem = getattr(props, "total_memory", getattr(props, "total_mem", 0)) / 1e9
        return f"CUDA: {name} ({mem:.1f} GB)"
    if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
        return "Apple MPS"
    return "CPU"


def _best_device() -> str:
    if torch.cuda.is_available():
        return "cuda"
    if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def _tensor_to_pil_list(tensor: torch.Tensor) -> List[Image.Image]:
    """–¢–µ–Ω–∑–æ—Ä [B, C, H, W] –≤ [-1,1] –∏–ª–∏ [0,1] ‚Üí —Å–ø–∏—Å–æ–∫ PIL."""
    if tensor is None or not isinstance(tensor, torch.Tensor):
        return []
    img = tensor.detach().cpu().float()
    if img.min() < -0.01 or img.max() > 1.01:
        img = (img / 2 + 0.5).clamp(0, 1)
    else:
        img = img.clamp(0, 1)
    images = []
    for i in range(img.shape[0]):
        arr = (img[i].permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)
        if arr.shape[-1] == 1:
            arr = arr.squeeze(-1)
        images.append(Image.fromarray(arr))
    return images


def _video_tensor_to_file(tensor: torch.Tensor, fps: float = 8.0) -> Optional[str]:
    """–¢–µ–Ω–∑–æ—Ä –≤–∏–¥–µ–æ [B,C,T,H,W] –∏–ª–∏ [C,T,H,W] ‚Üí –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª .mp4."""
    if tensor is None or not isinstance(tensor, torch.Tensor):
        return None
    try:
        import tempfile
        t = tensor.detach().cpu().float()
        if t.min() < -0.01 or t.max() > 1.01:
            t = (t / 2 + 0.5).clamp(0, 1)
        if t.dim() == 5:
            t = t[0]
        # [C, T, H, W] ‚Üí frames (T, H, W, C)
        t = t.permute(1, 2, 3, 0).numpy()
        t = (t * 255).clip(0, 255).astype(np.uint8)
        path = tempfile.mktemp(suffix=".mp4")
        try:
            import imageio
            imageio.mimwrite(path, t, fps=fps)
            return path
        except ImportError:
            return None
    except Exception:
        return None


def _audio_tensor_to_file(tensor: torch.Tensor, sr: int = 44100) -> Optional[Tuple[int, np.ndarray]]:
    """–¢–µ–Ω–∑–æ—Ä –∞—É–¥–∏–æ [B, C, T] –∏–ª–∏ [C, T] ‚Üí (sample_rate, np.ndarray)."""
    if tensor is None or not isinstance(tensor, torch.Tensor):
        return None
    try:
        a = tensor.detach().cpu().float().numpy()
        if a.ndim == 3:
            a = a[0]
        if a.ndim == 2:
            a = a.mean(axis=0)
        a = np.clip(a, -1, 1).astype(np.float32)
        return (sr, a)
    except Exception:
        return None


def _get_templates_by_modality() -> Dict[str, List[Tuple[str, str]]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {modality: [(template_id, description), ...]}."""
    try:
        from yggdrasil.pipeline import InferencePipeline
        available = InferencePipeline.list_available()
    except Exception:
        available = {}
    result = {"image": [], "video": [], "audio": []}
    for name, info in available.items():
        desc = (info.get("description") or name).strip().split("\n")[0][:80]
        mod = info.get("modality", "image")
        if mod not in result:
            result[mod] = []
        result[mod].append((name, desc))
    for mod in result:
        result[mod].sort(key=lambda x: x[0])
    if not any(result.values()):
        result["image"] = [("sd15_txt2img", "SD 1.5 Text-to-Image")]
    return result


# ==================== MAIN UI ====================

def create_ui(
    manager: Optional[Any] = None,
    config: Optional[ServerConfig] = None,
    share: bool = False,
) -> "gr.Blocks":
    """–°–æ–∑–¥–∞—Ç—å –µ–¥–∏–Ω—ã–π Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ª—é–±—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    import gradio as gr

    templates_by_mod = _get_templates_by_modality()
    device = _best_device()
    device_info = _get_device_info()

    # ---------- Generation logic ----------
    def run_generation(
        modality: str,
        template_name: str,
        prompt: str,
        negative_prompt: str,
        num_steps: int,
        guidance_scale: float,
        width: int,
        height: int,
        num_frames: int,
        seed: int,
        batch_size: int,
        control_image: Optional[Any],
        ip_image: Optional[Any],
        source_image: Optional[Any],
        extra_params_json: str,
        pipeline_state: Optional[Tuple[str, Any]],
    ) -> Tuple[
        Optional[List[Image.Image]],
        Optional[str],
        Optional[Tuple[int, np.ndarray]],
        str,
        Optional[Tuple[str, Any]],
    ]:
        """–ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (images, video_path, audio_tuple, info, new_pipeline_state)."""
        if not template_name or template_name not in [t[0] for t in templates_by_mod.get(modality, [])]:
            return [], None, None, "–í—ã–±–µ—Ä–∏—Ç–µ —à–∞–±–ª–æ–Ω –ø–∞–π–ø–ª–∞–π–Ω–∞.", pipeline_state

        try:
            from yggdrasil.pipeline import InferencePipeline
        except ImportError as e:
            return [], None, None, f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}", pipeline_state

        # Reuse pipeline if same template
        pipe = None
        if pipeline_state and pipeline_state[0] == template_name:
            pipe = pipeline_state[1]
        if pipe is None:
            try:
                pipe = InferencePipeline.from_template(template_name, device=device)
            except Exception as e:
                return [], None, None, f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞–π–ø–ª–∞–π–Ω: {e}", pipeline_state
            pipeline_state = (template_name, pipe)

        def _pil_to_tensor(pil_img) -> torch.Tensor:
            if pil_img is None:
                return None
            if isinstance(pil_img, dict) and "image" in pil_img:
                pil_img = pil_img["image"]
            arr = np.array(pil_img).astype(np.float32) / 255.0
            if arr.ndim == 2:
                arr = np.stack([arr] * 3, axis=-1)
            t = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0)
            return t

        actual_seed = int(seed) if seed >= 0 else random.randint(0, 2**32 - 1)
        kwargs = {
            "prompt": prompt or "a beautiful scene",
            "negative_prompt": negative_prompt or "",
            "num_steps": num_steps,
            "guidance_scale": guidance_scale,
            "width": width,
            "height": height,
            "seed": actual_seed,
            "batch_size": min(max(1, batch_size), 8),
        }
        if modality == "video":
            kwargs["num_frames"] = num_frames
        if control_image is not None:
            kwargs["control_image"] = _pil_to_tensor(control_image)
        if ip_image is not None:
            kwargs["ip_image"] = ip_image if isinstance(ip_image, dict) else {"image": ip_image}
        if source_image is not None and modality in ("video", "image"):
            kwargs["source_image"] = _pil_to_tensor(source_image)

        # G3: merge extra params from JSON (dynamic graph_inputs)
        kwargs = merge_extra_params_json(kwargs, extra_params_json or "")

        start = time.time()
        try:
            out = pipe(**kwargs)
        except Exception as e:
            return [], None, None, f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}", pipeline_state
        elapsed = time.time() - start

        images = None
        video_path = None
        audio_data = None
        if out.images:
            images = out.images
        if getattr(out, "video", None) is not None:
            video_path = _video_tensor_to_file(out.video)
        if getattr(out, "audio", None) is not None:
            audio_data = _audio_tensor_to_file(out.audio)

        info = f"Seed: {actual_seed} | Steps: {num_steps} | CFG: {guidance_scale} | {elapsed:.1f}s | {device_info}"
        return images or [], video_path, audio_data, info, pipeline_state

    def update_template_choices(modality: str):
        choices = templates_by_mod.get(modality, [])
        return gr.update(choices=[t[0] for t in choices], value=choices[0][0] if choices else None)

    def load_pipeline_and_show_inputs(
        template_name: str, current_state: Optional[Tuple[str, Any]]
    ) -> Tuple[Optional[Tuple[str, Any]], str]:
        """G3: Load pipeline without generating; return (state, markdown of graph_inputs)."""
        if not template_name:
            return current_state, "*–í—ã–±–µ—Ä–∏—Ç–µ —à–∞–±–ª–æ–Ω –ø–∞–π–ø–ª–∞–π–Ω–∞.*"
        try:
            from yggdrasil.pipeline import InferencePipeline
            pipe = InferencePipeline.from_template(template_name, device=device)
            if hasattr(pipe, "graph") and pipe.graph is not None:
                pipe.graph.to(device)
            new_state = (template_name, pipe)
            g = pipe.graph
            inps = getattr(g, "graph_inputs", None) or {}
            inputs_list = sorted(inps.keys()) if isinstance(inps, dict) else []
            meta = getattr(g, "metadata", None) or {}
            control_mapping = meta.get("controlnet_input_mapping") or {}
            for v in control_mapping.values():
                if v not in inputs_list:
                    inputs_list.append(v)
            inputs_list = sorted(set(inputs_list))
            md = "**–í—Ö–æ–¥—ã –≥—Ä–∞—Ñ–∞:** " + ", ".join(inputs_list) if inputs_list else "**–í—Ö–æ–¥—ã –≥—Ä–∞—Ñ–∞:** (–Ω–µ—Ç)"
            return new_state, md
        except Exception as e:
            return current_state, f"*–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}*"

    # ---------- Build UI ----------
    with gr.Blocks(
        title="YggDrasil ‚Äî Universal Diffusion",
        theme=gr.themes.Soft(
            primary_hue="indigo",
            secondary_hue="slate",
        ),
        css="""
        .hero { text-align: center; margin-bottom: 0.5em; font-size: 1.8em; }
        .sub { text-align: center; color: #64748b; margin-bottom: 1.2em; }
        .preset-btn { min-width: 4em; }
        .footer { text-align: center; margin-top: 1.5em; color: #94a3b8; font-size: 0.9em; }
        """,
    ) as demo:

        gr.HTML('<h1 class="hero">üå≥ YggDrasil</h1><p class="sub">–ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∏–¥–µ–æ –∏ –∑–≤—É–∫–∞</p>')

        pipeline_state = gr.State(value=None)

        with gr.Tabs():
            # ========== TAB 1: INFERENCE (9A.1) ==========
            with gr.Tab("üé® Inference", id="inference"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### –ú–æ–¥–µ–ª—å –∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å")
                        modality_radio = gr.Radio(
                            choices=[("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "image"), ("–í–∏–¥–µ–æ", "video"), ("–ê—É–¥–∏–æ", "audio")],
                            value="image",
                            label="–¢–∏–ø",
                            elem_id="modality",
                        )
                        template_dropdown = gr.Dropdown(
                            label="–ü–∞–π–ø–ª–∞–π–Ω",
                            choices=[t[0] for t in templates_by_mod["image"]],
                            value=templates_by_mod["image"][0][0] if templates_by_mod["image"] else None,
                            interactive=True,
                        )
                        num_frames_num = gr.Slider(4, 64, value=16, step=1, label="–ö–∞–¥—Ä–æ–≤ (–≤–∏–¥–µ–æ)", visible=False)
                        def on_modality(m):
                            choices = templates_by_mod.get(m, [])
                            vis = m == "video"
                            return (
                                gr.update(choices=[t[0] for t in choices], value=choices[0][0] if choices else None),
                                gr.update(visible=vis),
                            )
                        modality_radio.change(
                            fn=on_modality,
                            inputs=[modality_radio],
                            outputs=[template_dropdown, num_frames_num],
                        )

                        gr.Markdown("### –¢–µ–∫—Å—Ç –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
                        prompt_input = gr.Textbox(
                            label="Prompt",
                            placeholder="–æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–æ–ª—É—á–∏—Ç—å...",
                            lines=3,
                        )
                        negative_input = gr.Textbox(
                            label="Negative Prompt",
                            placeholder="—Ä–∞–∑–º—ã—Ç–æ, –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ...",
                            lines=2,
                        )
                        with gr.Row():
                            steps_num = gr.Slider(1, 150, value=28, step=1, label="–®–∞–≥–∏")
                            cfg_num = gr.Slider(0.5, 30, value=7.5, step=0.5, label="CFG")
                        with gr.Row():
                            width_num = gr.Slider(128, 2048, value=512, step=64, label="–®–∏—Ä–∏–Ω–∞")
                            height_num = gr.Slider(128, 2048, value=512, step=64, label="–í—ã—Å–æ—Ç–∞")
                        with gr.Row():
                            seed_num = gr.Number(label="Seed (-1 = —Å–ª—É—á–∞–π–Ω—ã–π)", value=-1, precision=0)
                            seed_random_btn = gr.Button("üé≤ –°–ª—É—á–∞–π–Ω—ã–π", size="sm")
                            batch_num = gr.Slider(1, 8, value=1, step=1, label="–ë–∞—Ç—á")
                        seed_random_btn.click(lambda: -1, outputs=[seed_num])

                        gr.Markdown("#### –ü—Ä–µ—Å–µ—Ç—ã")
                        with gr.Row():
                            gr.Button("512√ó512").click(lambda: (512, 512), outputs=[width_num, height_num])
                            gr.Button("768√ó768").click(lambda: (768, 768), outputs=[width_num, height_num])
                            gr.Button("1024√ó1024").click(lambda: (1024, 1024), outputs=[width_num, height_num])
                            gr.Button("–ë—ã—Å—Ç—Ä–æ (20 —à–∞–≥–æ–≤)").click(lambda: 20, outputs=[steps_num])
                            gr.Button("–ö–∞—á–µ—Å—Ç–≤–æ (40 —à–∞–≥–æ–≤)").click(lambda: 40, outputs=[steps_num])
                        gr.Markdown("#### –ê–¥–∞–ø—Ç–µ—Ä—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")
                        control_image_in = gr.Image(label="Control (depth/canny)", type="pil")
                        ip_image_in = gr.Image(label="IP-Adapter –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", type="pil")
                        source_image_in = gr.Image(label="–ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (img2vid)", type="pil")

                        # G3: dynamic visibility when template/modality changes (without page reload)
                        def on_template_or_modality(tpl, mod):
                            ctrl, ip, src = infer_input_visibility(tpl or "", mod or "image")
                            return gr.update(visible=ctrl), gr.update(visible=ip), gr.update(visible=src)

                        for inp in [template_dropdown, modality_radio]:
                            inp.change(
                                fn=on_template_or_modality,
                                inputs=[template_dropdown, modality_radio],
                                outputs=[control_image_in, ip_image_in, source_image_in],
                            )

                        gr.Markdown("#### –î–æ–ø. –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ graph_inputs (G3)")
                        extra_params_in = gr.Textbox(
                            label="–î–æ–ø. –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (JSON)",
                            placeholder='{"ip_adapter_scale": 0.5} ‚Äî –¥–ª—è –≤—Ö–æ–¥–æ–≤, –Ω–µ –æ—Ö–≤–∞—á–µ–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–æ–π –≤—ã—à–µ',
                            lines=2,
                            value="{}",
                        )
                        load_preview_btn = gr.Button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ –ø–æ–∫–∞–∑–∞—Ç—å –≤—Ö–æ–¥—ã", size="sm", variant="secondary")
                        graph_inputs_info = gr.Markdown(value="*–ù–∞–∂–º–∏—Ç–µ ¬´–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ –ø–æ–∫–∞–∑–∞—Ç—å –≤—Ö–æ–¥—ã¬ª –∏–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ ‚Äî –∑–¥–µ—Å—å –æ—Ç–æ–±—Ä–∞–∑—è—Ç—Å—è –≤—Ö–æ–¥—ã –≥—Ä–∞—Ñ–∞.*", visible=True)

                        gen_btn = gr.Button("üöÄ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å", variant="primary", size="lg")

                    with gr.Column(scale=2):
                        gr.Markdown("### –†–µ–∑—É–ª—å—Ç–∞—Ç")
                        out_gallery = gr.Gallery(
                            label="–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
                            columns=3,
                            height=500,
                            object_fit="contain",
                        )
                        out_video = gr.Video(label="–í–∏–¥–µ–æ", visible=False)
                        out_audio = gr.Audio(label="–ê—É–¥–∏–æ", visible=False)
                        gen_info = gr.Textbox(label="–ò–Ω—Ñ–æ", interactive=False)
                        download_btn = gr.DownloadButton(label="–°–∫–∞—á–∞—Ç—å –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç", visible=True)

                def run_and_show(
                    mod, tpl, prompt, neg, steps, cfg, w, h, nf, seed, batch,
                    ctrl_img, ip_img, src_img, extra_json, state,
                ):
                    images, video_path, audio_data, info, new_state = run_generation(
                        mod, tpl, prompt, neg, steps, cfg, w, h, nf, seed, batch,
                        ctrl_img, ip_img, src_img, extra_json, state,
                    )
                    vis_img = bool(images and len(images) > 0)
                    vis_vid = video_path is not None
                    vis_aud = audio_data is not None
                    # G3: show graph inputs when we have a materialized pipeline
                    inputs_md = graph_inputs_info.value
                    if new_state and len(new_state) >= 2 and hasattr(new_state[1], "graph") and new_state[1].graph is not None:
                        g = new_state[1].graph
                        inps = getattr(g, "graph_inputs", None) or {}
                        inputs_list = list(inps.keys()) if isinstance(inps, dict) else []
                        meta = getattr(g, "metadata", None) or {}
                        control_mapping = meta.get("controlnet_input_mapping") or {}
                        if control_mapping:
                            inputs_list.extend(control_mapping.values())
                        inputs_list = sorted(set(inputs_list))
                        inputs_md = "**–í—Ö–æ–¥—ã –≥—Ä–∞—Ñ–∞:** " + ", ".join(inputs_list) if inputs_list else "**–í—Ö–æ–¥—ã –≥—Ä–∞—Ñ–∞:** (–Ω–µ—Ç)"
                    # Download: first image as bytes or video path
                    download_file = None
                    if images and len(images) > 0:
                        buf = io.BytesIO()
                        images[0].save(buf, format="PNG")
                        buf.seek(0)
                        download_file = (buf.getvalue(), "yggdrasil_output.png")
                    elif video_path:
                        download_file = video_path
                    return (
                        gr.update(value=images or [], visible=vis_img),
                        gr.update(value=video_path, visible=vis_vid),
                        gr.update(value=audio_data, visible=vis_aud),
                        info,
                        new_state,
                        download_file,
                        inputs_md,
                    )

                load_preview_btn.click(
                    fn=load_pipeline_and_show_inputs,
                    inputs=[template_dropdown, pipeline_state],
                    outputs=[pipeline_state, graph_inputs_info],
                )

                gen_btn.click(
                    fn=run_and_show,
                    inputs=[
                        modality_radio, template_dropdown,
                        prompt_input, negative_input,
                        steps_num, cfg_num, width_num, height_num, num_frames_num,
                        seed_num, batch_num,
                        control_image_in, ip_image_in, source_image_in,
                        extra_params_in,
                        pipeline_state,
                    ],
                    outputs=[
                        out_gallery,
                        out_video,
                        out_audio,
                        gen_info,
                        pipeline_state,
                        download_btn,
                        graph_inputs_info,
                    ],
                )

            # ========== TAB 2: PIPELINE (9A.1 G2) ‚Äî —Å–±–æ—Ä–∫–∞ –∏ Materialize ==========
            with gr.Tab("üì¶ Pipeline", id="pipeline"):
                gr.Markdown("### –°–±–æ—Ä–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞")
                gr.Markdown("–í—ã–±–µ—Ä–∏—Ç–µ —à–∞–±–ª–æ–Ω –≥—Ä–∞—Ñ–∞ –∏ –Ω–∞–∂–º–∏—Ç–µ **Materialize** ‚Äî –≥—Ä–∞—Ñ –±—É–¥–µ—Ç —Å–æ–±—Ä–∞–Ω –∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ –≤–∫–ª–∞–¥–∫–∞—Ö Inference –∏ Train.")
                pipeline_template_dropdown = gr.Dropdown(
                    label="–®–∞–±–ª–æ–Ω –ø–∞–π–ø–ª–∞–π–Ω–∞",
                    choices=[t[0] for t in (templates_by_mod["image"] + templates_by_mod.get("video", []) + templates_by_mod.get("audio", []))],
                    value=templates_by_mod["image"][0][0] if templates_by_mod["image"] else None,
                )
                materialize_btn = gr.Button("‚ö° Materialize", variant="primary")
                pipeline_status = gr.Textbox(
                    label="–°—Ç–∞—Ç—É—Å",
                    value="–ù–∞–∂–º–∏—Ç–µ Materialize, —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ.",
                    interactive=False,
                    lines=3,
                )
                def do_materialize(template_name, current_state):
                    if not template_name:
                        return "–í—ã–±–µ—Ä–∏—Ç–µ —à–∞–±–ª–æ–Ω.", current_state, gr.update()
                    try:
                        from yggdrasil.pipeline import InferencePipeline
                        pipe = InferencePipeline.from_template(template_name, device=device)
                        if hasattr(pipe, "graph") and pipe.graph is not None:
                            pipe.graph.to(device)
                        new_state = (template_name, pipe)
                        msg = f"–ì—Ä–∞—Ñ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω: {template_name}. –£–∑–ª—ã: {list(pipe.graph.nodes.keys()) if getattr(pipe, 'graph', None) else '‚Äî'}. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–∫–ª–∞–¥–∫–∏ Inference –∏ Train."
                        # G3: update graph_inputs_info on Inference tab
                        g = getattr(pipe, "graph", None)
                        inps = getattr(g, "graph_inputs", None) or {} if g else {}
                        inputs_list = sorted(inps.keys()) if isinstance(inps, dict) else []
                        meta = getattr(g, "metadata", None) or {} if g else {}
                        for v in (meta.get("controlnet_input_mapping") or {}).values():
                            if v not in inputs_list:
                                inputs_list.append(v)
                        inputs_list = sorted(set(inputs_list))
                        inputs_md = "**–í—Ö–æ–¥—ã –≥—Ä–∞—Ñ–∞:** " + ", ".join(inputs_list) if inputs_list else "**–í—Ö–æ–¥—ã –≥—Ä–∞—Ñ–∞:** (–Ω–µ—Ç)"
                        return msg, new_state, gr.Markdown.update(value=inputs_md)
                    except Exception as e:
                        return f"–û—à–∏–±–∫–∞ Materialize: {e}", current_state, gr.update()
                materialize_btn.click(
                    fn=do_materialize,
                    inputs=[pipeline_template_dropdown, pipeline_state],
                    outputs=[pipeline_status, pipeline_state, graph_inputs_info],
                )

            # ========== TAB 4: BLOCKS (9A.1 G5) ‚Äî –∫–∞—Ç–∞–ª–æ–≥ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º ==========
            with gr.Tab("üß± Blocks", id="blocks"):
                gr.Markdown("### –ö–∞—Ç–∞–ª–æ–≥ –±–ª–æ–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (backbone, conditioner, adapter, solver, codec, ‚Ä¶)")
                def get_blocks_md():
                    try:
                        from yggdrasil.core.block.registry import list_blocks
                        blocks = list_blocks()
                        if not blocks:
                            return "–ù–µ—Ç –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤."
                        by_cat = {}
                        for k, cls in sorted(blocks.items()):
                            cat = k.split("/")[0] if "/" in k else "other"
                            by_cat.setdefault(cat, []).append((k, cls))
                        # G5: order by 9A categories (backbone, conditioner, adapter, solver, codec, segmenter, detector, ‚Ä¶)
                        cat_order = ("backbone", "conditioner", "adapter", "guidance", "solver", "codec", "loop", "schedule",
                                     "segmenter", "detector", "classifier", "depth_estimator", "pose_estimator", "super_resolution",
                                     "style_encoder", "feature_extractor", "loss", "graph", "processor", "diffusion")
                        lines = []
                        for cat in cat_order:
                            if cat not in by_cat:
                                continue
                            items = by_cat[cat]
                            lines.append(f"\n### {cat}")
                            for k, cls in items:
                                doc = (cls.__doc__ or "").split("\n")[0].strip()[:70]
                                lines.append(f"- `{k}` ‚Äî {doc}")
                        for cat, items in sorted(by_cat.items()):
                            if cat in cat_order:
                                continue
                            lines.append(f"\n### {cat}")
                            for k, cls in items:
                                doc = (cls.__doc__ or "").split("\n")[0].strip()[:70]
                                lines.append(f"- `{k}` ‚Äî {doc}")
                        return "\n".join(lines) or "–ù–µ—Ç –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤."
                    except Exception as e:
                        return f"–û—à–∏–±–∫–∞: {e}"
                blocks_md = gr.Markdown(value=get_blocks_md())
                gr.Button("–û–±–Ω–æ–≤–∏—Ç—å").click(fn=get_blocks_md, outputs=[blocks_md])

            # ========== TAB 3: TRAIN (9A.1 G4) ==========
            with gr.Tab("üéì Train", id="train"):
                gr.Markdown("### –û–±—É—á–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ / –¥–æ–æ–±—É—á–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞")
                gr.Markdown("–ü–æ—Å–ª–µ **Materialize** –Ω–∞ –≤–∫–ª–∞–¥–∫–µ Pipeline –Ω–∞–∂–º–∏—Ç–µ ¬´–£–∑–ª—ã –∏–∑ –≥—Ä–∞—Ñ–∞¬ª ‚Äî –ø–æ–¥—Å—Ç–∞–≤—è—Ç—Å—è –∏–º–µ–Ω–∞ —É–∑–ª–æ–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞.")
                with gr.Row():
                    with gr.Column():
                        train_template = gr.Dropdown(
                            label="–®–∞–±–ª–æ–Ω –≥—Ä–∞—Ñ–∞",
                            choices=[t[0] for t in (templates_by_mod["image"] + templates_by_mod.get("video", []) + templates_by_mod.get("audio", []))],
                            value=templates_by_mod["image"][0][0] if templates_by_mod["image"] else None,
                        )
                        with gr.Row():
                            train_nodes = gr.Textbox(label="–û–±—É—á–∞–µ–º—ã–µ —É–∑–ª—ã (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)", value="lora_adapter", placeholder="backbone, lora_adapter")
                            sync_nodes_btn = gr.Button("–£–∑–ª—ã –∏–∑ –≥—Ä–∞—Ñ–∞", size="sm")
                        train_data = gr.Textbox(label="–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º", placeholder="/path/to/images/")
                        train_epochs = gr.Slider(1, 500, value=10, step=1, label="–≠–ø–æ—Ö–∏")
                        train_lr = gr.Number(label="Learning rate", value=1e-4)
                    with gr.Column():
                        train_btn = gr.Button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ", variant="primary")
                        train_status = gr.Textbox(label="–°—Ç–∞—Ç—É—Å", interactive=False, lines=5)
                def sync_train_nodes_from_state(state):
                    """G4: fill train_nodes from materialized graph."""
                    if not state or len(state) < 2:
                        return gr.update()
                    pipe = state[1]
                    g = getattr(pipe, "graph", None)
                    if g is None or not hasattr(g, "nodes"):
                        return gr.update()
                    return ", ".join(g.nodes.keys())

                sync_nodes_btn.click(
                    fn=sync_train_nodes_from_state,
                    inputs=[pipeline_state],
                    outputs=[train_nodes],
                )

                def start_train(tpl, nodes, data, epochs, lr):
                    if not data or not Path(data).exists():
                        return f"–ü—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {data}"
                    try:
                        from yggdrasil.core.graph.graph import ComputeGraph
                        from yggdrasil.training.graph_trainer import GraphTrainer, GraphTrainingConfig
                        from yggdrasil.training.data import ImageFolderSource
                        g = ComputeGraph.from_template(tpl)
                        nlist = [n.strip() for n in nodes.split(",") if n.strip()]
                        cfg = GraphTrainingConfig(num_epochs=int(epochs), batch_size=1, learning_rate=float(lr))
                        trainer = GraphTrainer(graph=g, train_nodes=nlist, config=cfg)
                        ds = ImageFolderSource(data)
                        import threading
                        def run():
                            try:
                                trainer.train(ds)
                            except Exception as e:
                                print(f"Train error: {e}")
                        threading.Thread(target=run, daemon=True).start()
                        return f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ: {tpl}, —É–∑–ª—ã {nlist}, —ç–ø–æ—Ö {epochs}"
                    except Exception as e:
                        return f"–û—à–∏–±–∫–∞: {e}"
                train_btn.click(
                    fn=start_train,
                    inputs=[train_template, train_nodes, train_data, train_epochs, train_lr],
                    outputs=[train_status],
                )

            # ========== TAB 5: PHILOSOPHY (9A.1) ==========
            with gr.Tab("üìú Philosophy", id="philosophy"):
                gr.Markdown("""
### YggDrasil ‚Äî –µ–¥–∏–Ω—ã–π –¥–≤–∏–∂–æ–∫ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –ø–∞–π–ø–ª–∞–π–Ω–æ–≤

**–ü—Ä–∏–Ω—Ü–∏–ø—ã:**
- **–ì—Ä–∞—Ñ –∫–∞–∫ Lego:** –ø–∞–π–ø–ª–∞–π–Ω ‚Äî —ç—Ç–æ DAG –±–ª–æ–∫–æ–≤ (conditioner, backbone, solver, codec, –∞–¥–∞–ø—Ç–µ—Ä—ã). –°–æ–±–∏—Ä–∞–π—Ç–µ –ª—é–±–æ–π –ø–∞–π–ø–ª–∞–π–Ω –∏–∑ —É–∑–ª–æ–≤.
- **–û–¥–∏–Ω –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä:** –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —É–∑–ª–æ–≤, –∞–≤—Ç–æ—Å–±–æ—Ä–∫–∞ —Ü–∏–∫–ª–æ–≤ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞, –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ.
- **–õ—é–±–∞—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å –∏ –º–æ–¥–µ–ª—å:** –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ; Stable Diffusion, SDXL, Flux, Diffusers-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –∏ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.
- **–ï–¥–∏–Ω—ã–π API:** InferencePipeline –∏ TrainingPipeline –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ –æ–±—É—á–µ–Ω–∏—è; –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –≥—Ä–∞—Ñ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è.

**–°—Å—ã–ª–∫–∏:** —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π, –ø—Ä–∏–º–µ—Ä—ã –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è ‚Äî —Å–º. –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–∞.
                """)

        gr.HTML(f'<div class="footer">YggDrasil ‚Äî Lego –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–∏ ¬∑ {device_info}</div>')

        # G3: set initial adapter visibility on load
        demo.load(
            fn=on_template_or_modality,
            inputs=[template_dropdown, modality_radio],
            outputs=[control_image_in, ip_image_in, source_image_in],
        )

    return demo
